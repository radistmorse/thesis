\chapter{Analysis Software (ZeeD)}
\label{sec:ZeeD}
ZeeD (\Zee\ DESY) is a software solution, developed internally by the DESY ATLAS standard model group under the supervision of Dr. A. Glazov. It is based on the \Athena\ framework that is largely used in ATLAS, but uses it mostly for the input data reading, which is in the AOD format which is also standard for ATLAS. For performance reasons, the first step in the analysis is the conversion of the data in the ZeeD internal format called TTrees (which is actually the name of the class in the ROOT framework which is used in this format, but in this work any mention of TTrees means this data format). This conversion allows the substantial speedup of the analysis, as well as considerably reduce the space needed for the data storage. The later stages of the analysis include the selection (application of cuts), reweighting (in case of MC) and plotting. ZeeD also allows to apply the systematic shifts as well as use ToyMC for uncertainties propagation studies. All these stages and subsystems of ZeeD will be described in this section.

\section{TTrees}
\label{sec:ZeeD_TTrees}

The data from the ATLAS detector comes in the form of the AOD files, which a special format based on the ROOT data file format and developed internally by the ATLAS. The format allows the seamless reading and writing of the various data objects (e.g. tracks, EM clusters, reconstructed particles, etc) within the \Athena\ framework. In AOD files all the information reconstructed from the collision is stored. In case of simulated events (i.e. MC) there is also the information about the generated particles, the so-called "truth particles", which can be used to determine the efficiency of the reconstruction algorithms and the analysis software.

Most of the information stored in the AODs is unneeded for the \Zee\ analysis, which only needs reconstructed electrons. And not even all the information available for the electrons is needed, as we only need enough to apply cuts and construct various differential cross-sections. During the analysis workflow, ZeeD uses about $0.1\%$ of all the information stored in the AODs, but because of the framework structure, all the content of it should be read from the store and preloaded into the memory, which consumed most of the time needed for the analysis iteration. Also, because of the nature of the ATLAS data storages and the GRID farms used for the analysis, it was prone to errors, and it would require several attempts and consume much time to finish even one iteration.

The solution for this problem is to extract the needed information from the AOD files and store it in the simplified structure in ROOT ntuples. Similar solution was later used for the AOD successor - xAOD format. But whereas xAOD allows all this to be done automatically, for the 2011 the custom tool for manual conversion had to be developed. The software chain for the TTree writing is no different from the analysis chain. The data is read from the AODs, the preselection cuts are applied to reduce the amount of data, and the output information is written out, which in this case is the raw events.

Because of this similarity, the same software - ZeeD - was used for the TTrees production as well. It is capable of reading the data from AODs, it has all the cuts applicable for preselection, and it is capable of outputting any kind of data from the selected events. With the corresponding setup, the ZeeD workflow will be suitable for this production.

\section{Boson Finder}

The main target of all the analyses that are conducted using the ZeeD software are the Z bosons, so finding all the possible Z boson candidates in every event is always the first task. The ZeeD component that does that job is the boson finder class. On the basic level it just creates a boson candidate for every possible pair of electrons, but further in the analysis it keeps track of all the bosons and records all the intermediate results for each one. Ultimately, we won't accept more than one boson from any event, so every boson candidate is assigned a special "score" of how well it passes the various cuts. When there are several boson candidates that pass all the required cuts, the candidate with the highest score is picked for the final results. In case of the final \Zee\ analysis this situation is impossible, though, a we require not more than two suitable electrons. This mechanism is used for various tests and cross-checks.

\section{Cuts}

The analysis implications of different cuts will be discussed in the Sec.~\ref{sec:Sel_cuts}. Here we'll discuss the technical aspects of the event selection.

From the technical point of view, every cut is a restriction on one or several variables of the reconstructed event. Every event can pass or fail every cut, and the infrastructure should be able not only to provide information whether some event passed all the cuts or not (which is the only thing that is important for the analysis itself), but also to show the results of all the cuts separately. This is called a "cut-flow" and it is used for debugging purposes and cross-checking. In the situation when different groups using different software come to different results for the same analysis, the ability to find out on which cut every particular event was filtered out is very helpful. With it we can find out which cut gives different results across different analysis groups.

The other functionality which is needed from the cuts facility is the cut reversal which is used for the background estimation. The background estimation methods (discussed later in Sec.~\ref{sec:Bkg}) requires to select the events where electron pass some specific cuts, but fail some other specific cuts. To achieve that, the cuts framework construct the bit-mask for every event, where every bit corresponds to some cut, and displays whether that event passed or failed that cut. After the bit mask is composed, we can compare it to the desired bit-mask, and include or exclude the event from the analysis.

\section{Reweighting}

The reweighting of the MC events is used for the fine-tune of the simulation with relation to the real data. The weights are calculated based on the well defined distributions of the data (such as the trigger efficiencies) and are to bring the corresponding MC distributions in accordance with the data. The calculation of the weights is done using some well-measured process. For instance, \Zee\ decay is good for calculating the trigger efficiency, since it has a low background in the peak region, and has two electrons which allow the use of the tag and probe method. The calculation of the weights is thus done by conducting the same analysis on MC and real data without any corrections whatsoever, and then comparing the distributions, for which we want to introduce reweighting. Weights are usually applied in bins, so the result of the weight calculation would be a histogram, sometimes two-dimensional, which would cover the kinematic plane for the variable being reweighted. Apart from the weight itself, the statistical and systematical uncertainties for every bin are also calculated, and stored in the similar histograms.

After the weights are calculated, they are used during the analysis. The weights are picked for every event based on its properties, and then the final weight of the event is calculated by multiplying all the weights from different corrections. This weight is later used during the histograms plotting. Since every weight comes with the statistical and systematical uncertainties, they also contribute to the calculation of the final error for the analysis. The process of calculation of the impact of different uncertainties to the final combined error would be discussed in the sec.~\ref{sec:ZeeD_toymc}.

\section{Histogram managers}

The results of the analysis are the histograms, showing the various distributions of the physical quantities. The facilities responsible for that are the histogram managers. Each manager represents some aspect of the analysis, and creates and fills one or several histograms, which are thus grouped by their role in it. Each of the managers can be attached or detached during the launch setup, which allows to conduct various analyses with the same software, as long as this analysis deals with electrons, muons, and missing $E_{t}$ (the only kind of data that ZeeD extracts from the egamma data stream).

For the \Zee\ analysis the important managers would be an electron manager (plots the various distributions of the electrons properties, such as energy, $E_{t}$, geometrical variables, etc), a ZCF manager (plots the properties of the central-forward Z-bosons), and (in case of MC) the truth managers (plots the truth information for the given event).

There is also one special histogram manager, which actually doesn't plot any histograms, but uses the same interfaces. It is the ZeeD TTree writer, which dumps out the content of the event in the ROOT tuples. This histogram manager is used to produce the TTrees that were previously discussed in sec.~\ref{sec:ZeeD_TTrees}.

\section{Systematics and ToyMC}
\label{sec:ZeeD_toymc}

One of the most important aspects of the analysis is the error estimation. The calculation of the statistical error can be done analytically, since it is subjected to the law of the large numbers. Estimation of the systematical errors can't be done that way, because the propagation of the error through the analysis procedures is complex, and can't be described analytically. Same applies to the error propagation from the underlying parameters, such as efficiencies and corrections.

To propagate the errors, there was introduced the system of systematic shifts. The quantity, the effect of error of which is to be measured, is shifted up or down to the amount of it's combined error, and the resulting shift in the final distributions is measured as a contribution of this quantity's error to the final combined error. This system is easy, but it's inefficient in the situations of the relatively large statistical errors, because it treats them as systematical.

To properly propagate the statistical error, the system of ToyMC was introduced. The ToyMC system conducts many (usually several hundreds) of analyses, with the respectable quantities shifted within its combined error according to the Gauss distribution. In the output, we are getting the similar distribution of the final values, which can be used to properly estimate the final combined error. The evolution of the ToyMC method was the Combined ToyMC, which could be used to propagate both systematical and statistical errors simultaneously, thus instantly getting the combined error as an output.
