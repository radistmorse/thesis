\chapter{MC (Monte Carlo) Samples}
\label{sec:MCSamples}

\section{Frozen Showers}
\label{MC_FS}
The frozen showers system (FS) is a system that is designed to accelerate the geant4 simulation process inside the EM calorimeters. The main principle of FS is to substitute the low-energy particles with the EM-showers, which are pre-calculated and stored in the libraries. These libraries must be generated in advance for each calorimeter and for each particle which you want to parameterize during the production simulation. The generation process consists mostly of the simulating of the low-energy particles and saving the information about every energy deposition this particle made. The array of these deposits or "hits" passes through several post-processing procedures in order to reduce the size of it, and becomes a "shower".

The low-energy particles, from the simulation of which the showers are produced, must be generated themself first. The initial approach was to use a particle gun, to create particles of the given type in a fixed set of points on the $E$-$\eta$ kinematic plane. The resulting libraries were two-dimentianally binned, and the coverage of the whole kinematic plane was achieved using the interpolation. This approach had several disadvantages. First, it populated all the bins with the same amout of showers, while during the production the low-energy (0-10 MeV) showers were used a lot more often (by thousands of times) than the high-energy (200-1000 MeV) ones. This resulted in two problems: the overpopulated high-energy bins needlessly encreased the size of the library and the under-populated low-energy bins decreased the diversity of the simulation. One also needs to notice, that the size of the high-energy shower is 20 to 50 times larger than that of the low-energy one. Second disadvantage was the low diversity of the showers in the library. Every shower in the bin was produced based on the particle generated in the exact same point on the kinematic plane. As the partial solution to this problem, the so-called "eta-shaking" was introduced: the eta of the particle was randomly shifted, while still remained in the boundaries of the eta bin. But this solution didn't provide the needed diversity. And the third problem was the interpolation algorithm, which scaled the energy of the shower correctly, but couldn't scale the size of the shower or the number of hits in it, which also depend on the energy.

To solve all these problems, the new way to generate the low-energy particles was introduced, which was called "two-staged generation". First, we take several thousands of generated particles from the real MC sample ans start simulating them. During the simulation, every time when the low-energy particle is requested to be parameterized using frozen showers, we save the parameters of this low-energy particle as a starting point for the shower. As the starting points tend to be clustered very tightly around the track of the initial high-energy particle, only the fracture of the initial starting points is used for library generation in order to rarify them and get a more even coverage of the detector's volume. The second stage is the simulation itself. During this stage we take the aforementioned starting points and produce the showers the same way we did with the particle gun.

This new way of library generation solved all the problems mentioned above. The bin population problem does not occure because we produce the showers based on the real MC samples, and the resulting size of each bin is in direct dependence of the number of times this bin is used. This allowed us to reduce the size of the library by the factor of two, and yet make a bigger diversity. The interpolation issue was also solved by generating the libraries with continuous energy distribution. This way, the library can always return a shower with all the energy-dependent parameters very close to the requested. In the heavily populated bins the error could be a fraction of the per mille.

The new way of generation of the showers made new demands to the libraries, which in turn required some changes to the frozen showers system itself, which ultimately leaded to the pretty much rewriting of the whole project. The libraries with the continuous energy distribution were the biggest problem, but there was also other problems, like the fixed 2D binning of the libraries, with the very strong emphasis on the $E$-$\eta$ binning. In case of any other binning, the system required lots of special non-intuitive code. There were also problems which were not relevant to the libraries itself, but had an easy solution with some tweaks to the libraries format, which was overhauled anyway. One example is the containment check, which is the check that the parameterized particle is deep enough inside the detector, and hence the substituting shower will be fully inside it as well. The check is conducted by substituing the particle with the "average-sized" shower, and seeing if this shower is fully inside the calorimeter. Previously, the "average size of the shower" was calculated based on some magic numbers and algorithms, which nobody knew how they worked. With the new system it became possible to calculate the averge sizes of the showers based on the library itself, and store them in that very library.

Starting from MC11b, frozen showers system was enabled by default in FCAL. Several studies showed that the errors introduced by the FS is negligible compared to the errors between MC and data, while the simulation speedup was about 25\%. The smallness of the errors introduced by the FS compared to other fast simulation methods (most notably - FastCaloSim) leaded to the misunderstanding, when the simulation with the FS system enabled was called "full simulation" as opposed to other, less precise but faster methods, which were called "fast simulation".
